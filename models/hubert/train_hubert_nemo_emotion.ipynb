{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2036183f",
   "metadata": {},
   "source": [
    "# Fine-tune HuBERT for Emotion Recognition on nEMO\n",
    "This notebook uses Hugging Face Transformers to fine-tune a HuBERT model for speech emotion recognition on the nEMO dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f11a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:30.284476400Z",
     "start_time": "2025-05-25T14:36:31.125726Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    HubertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "042a7efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:30.329053500Z",
     "start_time": "2025-05-25T14:36:45.695875Z"
    }
   },
   "source": [
    "model_name_or_path = 'facebook/hubert-base-ls960'\n",
    "dataset_name = 'amu-cai/nEMO'\n",
    "output_dir = './hubert-nemo-emotion'\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "learning_rate = 3e-5\n",
    "num_train_epochs = 5\n",
    "eval_steps = 200\n",
    "save_steps = 200"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1aa67ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:30.348051900Z",
     "start_time": "2025-05-25T14:36:45.704496Z"
    }
   },
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch['audio']['array']\n",
    "    features = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16_000,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch['input_values'] = features.input_values[0].numpy().tolist()\n",
    "    batch['attention_mask'] = features.attention_mask[0].numpy().tolist()\n",
    "    batch['labels'] = label2id[batch['emotion']]\n",
    "    return batch"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0d6d02dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:30.362052500Z",
     "start_time": "2025-05-25T14:36:45.716581Z"
    }
   },
   "source": [
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    acc = accuracy_score(pred.label_ids, preds)\n",
    "    f1 = f1_score(pred.label_ids, preds, average='macro')\n",
    "    return {'accuracy': acc, 'f1_macro': f1}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "efdccaf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:30.364052500Z",
     "start_time": "2025-05-25T14:36:45.729062Z"
    }
   },
   "source": [
    "raw = load_dataset(dataset_name, split='train')\n",
    "ds = raw.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, eval_ds = ds['train'], ds['test']\n",
    "train_ds = train_ds.cast_column('audio', Audio(sampling_rate=16_000))\n",
    "eval_ds = eval_ds.cast_column('audio', Audio(sampling_rate=16_000))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "92b66b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:36:51.392630Z",
     "start_time": "2025-05-25T14:36:49.878101Z"
    }
   },
   "source": [
    "\n",
    "unique_emotions = sorted(set(train_ds['emotion']))\n",
    "label2id = {emo: i for i, emo in enumerate(unique_emotions)}\n",
    "id2label = {i: emo for emo, i in label2id.items()}\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    sampling_rate=16_000,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "model = HubertForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(unique_emotions),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    problem_type='single_label_classification',\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ef649e55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:36:51.894172Z",
     "start_time": "2025-05-25T14:36:51.401731Z"
    }
   },
   "source": [
    "train_ds = train_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "eval_ds = eval_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=eval_ds.column_names\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "88bbd515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:36:52.213421Z",
     "start_time": "2025-05-25T14:36:51.903565Z"
    }
   },
   "source": [
    "data_collator = DataCollatorWithPadding(feature_extractor, padding=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=10000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5fe5e0f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:00:29.275363Z",
     "start_time": "2025-05-25T14:36:52.227449Z"
    }
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Temp\\ipykernel_4076\\3640001530.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 801/2520 22:34 < 48:34, 0.59 it/s, Epoch 1.59/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.260865</td>\n",
       "      <td>0.492205</td>\n",
       "      <td>0.437525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.945259</td>\n",
       "      <td>0.628062</td>\n",
       "      <td>0.568439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639579</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.772459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374178</td>\n",
       "      <td>0.893096</td>\n",
       "      <td>0.889634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:659] . unexpected pos 527565056 vs 527564948",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\torch\\serialization.py:965\u001B[39m, in \u001B[36msave\u001B[39m\u001B[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[39m\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[32m--> \u001B[39m\u001B[32m965\u001B[39m     \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m        \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    972\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\torch\\serialization.py:1266\u001B[39m, in \u001B[36m_save\u001B[39m\u001B[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[39m\n\u001B[32m   1265\u001B[39m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1266\u001B[39m \u001B[43mzip_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: [enforce fail at inline_container.cc:862] . PytorchStreamWriter failed writing file data/181: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      1\u001B[39m trainer = Trainer(\n\u001B[32m      2\u001B[39m     model=model,\n\u001B[32m      3\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     compute_metrics=compute_metrics,\n\u001B[32m      9\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m trainer.save_model(output_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\transformers\\trainer.py:2240\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2238\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2245\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\transformers\\trainer.py:2622\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2620\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.epoch = epoch + (step + \u001B[32m1\u001B[39m + steps_skipped) / steps_in_epoch\n\u001B[32m   2621\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_step_end(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n\u001B[32m-> \u001B[39m\u001B[32m2622\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2623\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2624\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2625\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2626\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2627\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2628\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2629\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2630\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2631\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2632\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2633\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_substep_end(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\transformers\\trainer.py:3102\u001B[39m, in \u001B[36mTrainer._maybe_log_save_evaluate\u001B[39m\u001B[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001B[39m\n\u001B[32m   3099\u001B[39m         \u001B[38;5;28mself\u001B[39m.control.should_save = is_new_best_metric\n\u001B[32m   3101\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.control.should_save:\n\u001B[32m-> \u001B[39m\u001B[32m3102\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3103\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_save(\u001B[38;5;28mself\u001B[39m.args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\transformers\\trainer.py:3210\u001B[39m, in \u001B[36mTrainer._save_checkpoint\u001B[39m\u001B[34m(self, model, trial)\u001B[39m\n\u001B[32m   3206\u001B[39m         \u001B[38;5;28mself\u001B[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001B[32m   3208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.save_only_model:\n\u001B[32m   3209\u001B[39m     \u001B[38;5;66;03m# Save optimizer and scheduler\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3210\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_optimizer_and_scheduler\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3211\u001B[39m     \u001B[38;5;28mself\u001B[39m._save_scaler(output_dir)\n\u001B[32m   3212\u001B[39m     \u001B[38;5;66;03m# Save RNG state\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\transformers\\trainer.py:3337\u001B[39m, in \u001B[36mTrainer._save_optimizer_and_scheduler\u001B[39m\u001B[34m(self, output_dir)\u001B[39m\n\u001B[32m   3332\u001B[39m     save_fsdp_optimizer(\n\u001B[32m   3333\u001B[39m         \u001B[38;5;28mself\u001B[39m.accelerator.state.fsdp_plugin, \u001B[38;5;28mself\u001B[39m.accelerator, \u001B[38;5;28mself\u001B[39m.optimizer, \u001B[38;5;28mself\u001B[39m.model, output_dir\n\u001B[32m   3334\u001B[39m     )\n\u001B[32m   3335\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.should_save:\n\u001B[32m   3336\u001B[39m     \u001B[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3337\u001B[39m     \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOPTIMIZER_NAME\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3339\u001B[39m \u001B[38;5;66;03m# Save SCHEDULER & SCALER\u001B[39;00m\n\u001B[32m   3340\u001B[39m is_deepspeed_custom_scheduler = \u001B[38;5;28mself\u001B[39m.is_deepspeed_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m   3341\u001B[39m     \u001B[38;5;28mself\u001B[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001B[32m   3342\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\torch\\serialization.py:964\u001B[39m, in \u001B[36msave\u001B[39m\u001B[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[39m\n\u001B[32m    961\u001B[39m     f = os.fspath(f)\n\u001B[32m    963\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[32m--> \u001B[39m\u001B[32m964\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[32m    965\u001B[39m         _save(\n\u001B[32m    966\u001B[39m             obj,\n\u001B[32m    967\u001B[39m             opened_zipfile,\n\u001B[32m   (...)\u001B[39m\u001B[32m    970\u001B[39m             _disable_byteorder_record,\n\u001B[32m    971\u001B[39m         )\n\u001B[32m    972\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Polish-Speech-Emotion-Recognition\\venv\\Lib\\site-packages\\torch\\serialization.py:798\u001B[39m, in \u001B[36m_open_zipfile_writer_file.__exit__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    797\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m798\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfile_like\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite_end_of_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    799\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.file_stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    800\u001B[39m         \u001B[38;5;28mself\u001B[39m.file_stream.close()\n",
      "\u001B[31mRuntimeError\u001B[39m: [enforce fail at inline_container.cc:659] . unexpected pos 527565056 vs 527564948"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de6012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
